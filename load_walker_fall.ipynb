{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pybullet_envs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# import panda_gym # Not in requirements, required for panda envs, get latest from here: https://github.com/qgallouedec/panda-gym , don't use version from pypy \n",
    "\n",
    "from seagul.zoo3_utils import load_zoo_agent, ALGOS, do_rollout_stable\n",
    "\n",
    "path_to_zoo = \"/home/sgillen/work/external/rl-baselines3-zoo/\" # Very hacky but this is what we do for now. use: git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "\n",
    "model_dir = \"./keep_agents/train_all/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through save directory and load everything\n",
    "\n",
    "model_dict = {}\n",
    "env_dict = {}\n",
    "\n",
    "for algo_file in os.scandir(model_dir):\n",
    "    algo_dir = algo_file.path\n",
    "    algo = algo_dir.split(\"/\")[-1]\n",
    "    model_dict[algo] = {}\n",
    "    env_dict[algo] = {}\n",
    "    for env_file in os.scandir(algo_dir):\n",
    "        env_dir = env_file.path\n",
    "        env_name = env_dir.split(\"/\")[-1]\n",
    "        model_dict[algo][env_name] = {}\n",
    "        env_dict[algo][env_name] = {}\n",
    "        env, original_model = load_zoo_agent(env_name, algo, zoo_path = path_to_zoo)\n",
    "        env_dict[algo][env_name] = env\n",
    "        model_dict[algo][env_name]['original'] = original_model\n",
    "        for pkl_file in os.scandir(env_dir):\n",
    "            post_name = pkl_file.path.split(\"/\")[-1].split(\".\")[0]\n",
    "            model_dict[algo][env_name][post_name] = ALGOS[algo].load(pkl_file.path, env=env, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_list = list(model_dict.keys())\n",
    "env_list = list(model_dict[algo_list[0]].keys())\n",
    "post_list = list(model_dict[algo_list[0]][env_list[0]].keys())\n",
    "\n",
    "print(algo_list) # Original Algorithm\n",
    "print(env_list) # Environment\n",
    "\n",
    "# Postprocessor, autogenerated names. \"original\" means the original agent from zoo without additional training, \"postprocess_default\" means just ARS with not extra reward.\n",
    "# Any other name is some new reward functio \n",
    "print(post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rollout Walker for no noise for n trials\n",
    "env_name = 'Walker2DBulletEnv-v0'\n",
    "data_no_noise={}\n",
    "#row = 0\n",
    "post_name = ['original', 'refined']\n",
    "env_id = 'Walker'\n",
    "for k in enumerate(post_list):\n",
    "    #for j in enumerate(env_list):\n",
    "    row = 0\n",
    "    for i in enumerate(algo_list):\n",
    "\n",
    "        env = env_dict[i[1]][env_name]\n",
    "        model = model_dict[i[1]][env_name][k[1]] # no refinement\n",
    "        #model2 = model_dict[i[1]][j[1]][post_list[1]] # ARS policy refinement\n",
    "        count = 1\n",
    "        fail = 0\n",
    "        rewards_list = []\n",
    "        l_list = [] \n",
    "        n_trial = 300 #Trial number\n",
    "        while count <= n_trial :\n",
    "            obs,act,rew,info = do_rollout_stable(env, model)\n",
    "            ep_rew = info[0]['episode']['r']\n",
    "            ep_l = info[0]['episode']['l']\n",
    "\n",
    "            rewards_list.append(ep_rew)\n",
    "            l_list.append(ep_l)\n",
    "\n",
    "            rewards_arr = np.stack(rewards_list).squeeze()\n",
    "            l_arr = np.stack(l_list).squeeze()\n",
    "\n",
    "            if info[0]['episode']['l']<1000:\n",
    "                fail+=1\n",
    "            count+=1\n",
    "        fail_percentage = 100*fail/n_trial\n",
    "\n",
    "        reward_mean = np.mean(rewards_arr)\n",
    "        reward_std = np.std(rewards_arr)\n",
    "        l_mean = np.mean(l_arr)\n",
    "        if k[0] == 1:\n",
    "            data_no_noise[str(row)] = data_no_noise[str(row)]+[reward_mean,reward_std,fail_percentage,l_mean]\n",
    "        else:\n",
    "            data_no_noise[str(row)] = [env_id,i[1],reward_mean,reward_std,fail_percentage,l_mean]\n",
    "\n",
    "        #data_no_noise[str(row)] = [k[1],env_name,i[1],reward_mean,reward_std,fail_percentage,l_mean]\n",
    "        print(row)\n",
    "        row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_no_noise['0'] = data_no_noise['0']+[reward_mean,reward_std,fail_percentage,l_mean]\n",
    "#print(data_no_noise)\n",
    "import pandas as pd\n",
    "#df=pd.DataFrame.from_dict(data_no_noise, orient='index', columns=['env', 'algo', 'post', 'r_mean', 'r_std', '%fail', 'l_mean'])\n",
    "df_no_noise = pd.DataFrame.from_dict(data_no_noise, orient='index', columns=['env', 'algo', 'r_mean', 'r_std', 'fall/100ep before', 'l_mean', 'r_mean_refined', 'r_std_refined', 'fail/100ep after', 'l_mean_refined'])\n",
    "\n",
    "# Save all as csv\n",
    "df_no_noise['algo'] = df_no_noise['algo'].str.upper()\n",
    "#path = '~/Documents/GitHub/policy_refinement/Ty_files/Ty_csv/walker_no_noise.csv'\n",
    "#df_no_noise.to_csv(path,index = False)\n",
    "\n",
    "# Save just the fall per 100 episode as csv\n",
    "df_no_noise2 = df_no_noise.drop(['r_mean', 'r_std','l_mean', 'r_mean_refined', 'r_std_refined','l_mean_refined'],axis=1)\n",
    "df_no_noise2['algo'] = df_no_noise2['algo'].str.upper()\n",
    "df_no_noise2 = df_no_noise2.round({ 'fall/100ep before':2, 'fall/100ep after':2})\n",
    "a,b, c, d, e,f = df_no_noise2.iloc[0].copy(), df_no_noise2.iloc[1].copy(),df_no_noise2.iloc[2].copy(), df_no_noise2.iloc[3].copy(),df_no_noise2.iloc[4].copy(), df_no_noise2.iloc[5].copy()\n",
    "df_no_noise2.iloc[0],df_no_noise2.iloc[1],df_no_noise2.iloc[2],df_no_noise2.iloc[3],df_no_noise2.iloc[4],df_no_noise2.iloc[5] = c,f,e,a,d,b\n",
    "\n",
    "#path = '~/Documents/GitHub/policy_refinement/Ty_files/Ty_csv/walker_fall_only_no_noise.csv'\n",
    "#df_no_noise2.to_csv(path,index = False)\n",
    "\n",
    "df_no_noise2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mujoco 1.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
